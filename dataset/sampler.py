# https://github.com/KaiyuYue/mgd/blob/master/mgd/sampler.py

import math
import torch
import torch.distributed as dist

class ExtraDistributedSampler(torch.utils.data.Sampler):
    """
    Extra distributed sampler for MGD update in DDP mode.

    This sampler makes sure that all ranks see the exactly same batched data
    in each iteration for updating flow matrix of MGD.

    Refer to https://pytorch.org/docs/stable/_modules/torch/utils/data/distributed.html#DistributedSampler.
    """
    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True):
        if num_replicas is None:
            if not dist.is_available():
                raise RuntimeError("Requires distributed package to be available")
            num_replicas = dist.get_world_size()
        if rank is None:
            if not dist.is_available():
                raise RuntimeError("Requires distributed package to be available")
            rank = dist.get_rank()
        self.dataset = dataset
        self.num_replicas = 1 # here we force num_replicas=1 for all ranks
        self.rank = rank
        self.epoch = 0
        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))
        self.total_size = self.num_samples * self.num_replicas
        self.shuffle = shuffle

    def __iter__(self):
        # deterministically shuffle based on epoch
        g = torch.Generator()
        g.manual_seed(self.epoch)
        if self.shuffle:
            indices = torch.randperm(len(self.dataset), generator=g).tolist()
        else:
            indices = list(range(len(self.dataset)))

        # add extra samples to make it evenly divisible
        indices += indices[:(self.total_size - len(indices))]
        assert len(indices) == self.total_size
        assert len(indices) == self.num_samples

        return iter(indices)

    def __len__(self):
        return self.num_samples

    def set_epoch(self, epoch):
        self.epoch = epoch

